nohup: ignoring input
Finetuning starting.
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards:  50%|█████     | 1/2 [01:11<01:11, 71.41s/it]Downloading shards: 100%|██████████| 2/2 [01:54<00:00, 54.94s/it]Downloading shards: 100%|██████████| 2/2 [01:54<00:00, 57.47s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.12s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.20s/it]
trainable params: 10,644,480 || all params: 6,901,373,952 || trainable%: 0.1542
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 7876 examples [00:00, 14401.01 examples/s]Generating train split: 7876 examples [00:00, 14177.22 examples/s]
Generating valid split: 0 examples [00:00, ? examples/s]Generating valid split: 1000 examples [00:00, 10145.70 examples/s]
Map:   0%|          | 0/7876 [00:00<?, ? examples/s]Map:  13%|█▎        | 1000/7876 [00:00<00:05, 1207.48 examples/s]Map:  25%|██▌       | 2000/7876 [00:01<00:04, 1238.39 examples/s]Map:  38%|███▊      | 3000/7876 [00:02<00:03, 1265.59 examples/s]Map:  51%|█████     | 4000/7876 [00:03<00:03, 1269.63 examples/s]Map:  63%|██████▎   | 5000/7876 [00:03<00:02, 1295.90 examples/s]Map:  76%|███████▌  | 6000/7876 [00:04<00:01, 1307.48 examples/s]Map:  89%|████████▉ | 7000/7876 [00:05<00:00, 1313.97 examples/s]Map: 100%|██████████| 7876/7876 [00:06<00:00, 1299.32 examples/s]Map: 100%|██████████| 7876/7876 [00:06<00:00, 1284.77 examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1344.30 examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1315.94 examples/s]
[2024-09-07 21:23:17,439] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
df: /root/.triton/autotune: No such file or directory
/workspace/venv/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/workspace/venv/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/workspace/venv/lib/python3.10/site-packages/transformers/trainer.py:3108: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)
wandb: Currently logged in as: tcwong. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.7
wandb: Run data is saved locally in /workspace/wandb/run-20240907_212339-xjbmhng9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deepseek_prover_base_no_err-by_file-09-07-08-59
wandb: ⭐️ View project at https://wandb.ai/tcwong/decoder
wandb: 🚀 View run at https://wandb.ai/tcwong/decoder/runs/xjbmhng9
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
  0%|          | 0/492 [00:00<?, ?it/s]/workspace/venv/lib/python3.10/site-packages/transformers/trainer.py:2843: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint_rng_state = torch.load(rng_file)
W0907 21:23:56.934000 133425874043328 torch/_dynamo/variables/tensor.py:715] [17/0] Graph break from `Tensor.item()`, consider setting:
W0907 21:23:56.934000 133425874043328 torch/_dynamo/variables/tensor.py:715] [17/0]     torch._dynamo.config.capture_scalar_outputs = True
W0907 21:23:56.934000 133425874043328 torch/_dynamo/variables/tensor.py:715] [17/0] or:
W0907 21:23:56.934000 133425874043328 torch/_dynamo/variables/tensor.py:715] [17/0]     env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1
W0907 21:23:56.934000 133425874043328 torch/_dynamo/variables/tensor.py:715] [17/0] to include these operations in the captured graph.
W0907 21:23:56.934000 133425874043328 torch/_dynamo/variables/tensor.py:715] [17/0] 
The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.
/workspace/venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 71%|███████▏  | 351/492 [02:36<01:02,  2.24it/s] 72%|███████▏  | 352/492 [04:31<02:07,  1.10it/s] 72%|███████▏  | 353/492 [06:25<03:36,  1.56s/it] 72%|███████▏  | 354/492 [08:18<05:41,  2.48s/it] 72%|███████▏  | 355/492 [10:12<08:34,  3.76s/it]                                                  72%|███████▏  | 355/492 [10:12<08:34,  3.76s/it] 72%|███████▏  | 356/492 [12:07<12:35,  5.55s/it] 73%|███████▎  | 357/492 [14:01<18:00,  8.00s/it] 73%|███████▎  | 358/492 [15:55<25:17, 11.32s/it] 73%|███████▎  | 359/492 [17:49<34:51, 15.73s/it] 73%|███████▎  | 360/492 [19:43<47:02, 21.38s/it]                                                  73%|███████▎  | 360/492 [19:43<47:02, 21.38s/it] 73%|███████▎  | 361/492 [21:38<1:02:09, 28.47s/it] 74%|███████▎  | 362/492 [23:32<1:19:50, 36.85s/it] 74%|███████▍  | 363/492 [25:26<1:39:38, 46.34s/it] 74%|███████▍  | 364/492 [27:20<2:00:20, 56.41s/it] 74%|███████▍  | 365/492 [29:13<2:20:41, 66.47s/it]                                                    74%|███████▍  | 365/492 [29:13<2:20:41, 66.47s/it] 74%|███████▍  | 366/492 [31:07<2:39:36, 76.00s/it] 75%|███████▍  | 367/492 [33:01<2:55:56, 84.45s/it] 75%|███████▍  | 368/492 [34:56<3:09:22, 91.64s/it] 75%|███████▌  | 369/492 [36:50<3:19:38, 97.38s/it]/workspace/venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 75%|███████▌  | 370/492 [38:49<3:29:50, 103.20s/it]                                                     75%|███████▌  | 370/492 [38:49<3:29:50, 103.20s/it] 75%|███████▌  | 371/492 [40:43<3:34:08, 106.18s/it] 76%|███████▌  | 372/492 [42:37<3:36:57, 108.48s/it] 76%|███████▌  | 373/492 [44:31<3:38:20, 110.09s/it] 76%|███████▌  | 374/492 [46:25<3:38:58, 111.34s/it] 76%|███████▌  | 375/492 [48:20<3:38:52, 112.25s/it]                                                     76%|███████▌  | 375/492 [48:20<3:38:52, 112.25s/it] 76%|███████▋  | 376/492 [50:13<3:37:47, 112.65s/it] 77%|███████▋  | 377/492 [52:07<3:36:35, 113.00s/it] 77%|███████▋  | 378/492 [54:01<3:35:03, 113.19s/it] 77%|███████▋  | 379/492 [55:55<3:33:32, 113.39s/it] 77%|███████▋  | 380/492 [57:49<3:32:12, 113.68s/it]                                                     77%|███████▋  | 380/492 [57:49<3:32:12, 113.68s/it] 77%|███████▋  | 381/492 [59:43<3:30:43, 113.90s/it] 78%|███████▊  | 382/492 [1:01:37<3:28:44, 113.86s/it] 78%|███████▊  | 383/492 [1:03:32<3:27:21, 114.14s/it] 78%|███████▊  | 384/492 [1:05:26<3:25:30, 114.17s/it] 78%|███████▊  | 385/492 [1:07:20<3:23:34, 114.15s/it]                                                       78%|███████▊  | 385/492 [1:07:20<3:23:34, 114.15s/it] 78%|███████▊  | 386/492 [1:09:14<3:21:31, 114.07s/it] 79%|███████▊  | 387/492 [1:11:09<3:19:50, 114.20s/it] 79%|███████▉  | 388/492 [1:13:03<3:18:02, 114.25s/it] 79%|███████▉  | 389/492 [1:14:57<3:16:07, 114.25s/it] 79%|███████▉  | 390/492 [1:16:52<3:14:16, 114.28s/it]                                                       79%|███████▉  | 390/492 [1:16:52<3:14:16, 114.28s/it] 79%|███████▉  | 391/492 [1:18:46<3:12:23, 114.29s/it] 80%|███████▉  | 392/492 [1:20:40<3:10:24, 114.24s/it] 80%|███████▉  | 393/492 [1:22:34<3:08:19, 114.13s/it] 80%|████████  | 394/492 [1:24:28<3:06:14, 114.03s/it] 80%|████████  | 395/492 [1:26:22<3:04:19, 114.02s/it]                                                       80%|████████  | 395/492 [1:26:22<3:04:19, 114.02s/it] 80%|████████  | 396/492 [1:28:15<3:02:14, 113.90s/it] 81%|████████  | 397/492 [1:30:09<3:00:15, 113.84s/it] 81%|████████  | 398/492 [1:32:03<2:58:27, 113.91s/it] 81%|████████  | 399/492 [1:33:57<2:56:29, 113.87s/it] 81%|████████▏ | 400/492 [1:35:51<2:54:35, 113.87s/it]                                                       81%|████████▏ | 400/492 [1:35:51<2:54:35, 113.87s/it]{'loss': 0.1051, 'grad_norm': 0.12189479172229767, 'learning_rate': 2.1889952770883643e-05, 'epoch': 2.89}
{'loss': 0.1066, 'grad_norm': 0.09821608662605286, 'learning_rate': 2.043849824442124e-05, 'epoch': 2.93}
{'loss': 0.1076, 'grad_norm': 0.0908147543668747, 'learning_rate': 1.9024375265982384e-05, 'epoch': 2.97}
{'loss': 0.1134, 'grad_norm': 0.11563821136951447, 'learning_rate': 1.764936965146773e-05, 'epoch': 3.01}
{'loss': 0.1081, 'grad_norm': 0.10024692863225937, 'learning_rate': 1.631521781767214e-05, 'epoch': 3.05}
{'loss': 0.0996, 'grad_norm': 0.11522582173347473, 'learning_rate': 1.502360458946232e-05, 'epoch': 3.09}
{'loss': 0.1155, 'grad_norm': 0.1005687341094017, 'learning_rate': 1.3776161072106702e-05, 'epoch': 3.13}
{'loss': 0.1184, 'grad_norm': 0.10551727563142776, 'learning_rate': 1.257446259144494e-05, 'epoch': 3.17}
{'loss': 0.1071, 'grad_norm': 0.09149405360221863, 'learning_rate': 1.1420026704498077e-05, 'epoch': 3.21}
{'loss': 0.0984, 'grad_norm': 0.08743679523468018, 'learning_rate': 1.031431128303153e-05, 'epoch': 3.25}

  0%|          | 0/125 [00:00<?, ?it/s][A
  2%|▏         | 2/125 [00:04<04:22,  2.14s/it][A
  2%|▏         | 3/125 [00:08<06:23,  3.14s/it][A
  3%|▎         | 4/125 [00:13<07:23,  3.66s/it][A
  4%|▍         | 5/125 [00:17<07:55,  3.97s/it][A
  5%|▍         | 6/125 [00:22<08:17,  4.18s/it][A
  6%|▌         | 7/125 [00:27<08:27,  4.30s/it][A
  6%|▋         | 8/125 [00:31<08:33,  4.39s/it][A
  7%|▋         | 9/125 [00:36<08:37,  4.46s/it][A
  8%|▊         | 10/125 [00:40<08:37,  4.50s/it][A
  9%|▉         | 11/125 [00:45<08:35,  4.52s/it][A
 10%|▉         | 12/125 [00:49<08:32,  4.53s/it][A
 10%|█         | 13/125 [00:54<08:30,  4.56s/it][A
 11%|█         | 14/125 [00:59<08:29,  4.59s/it][A
 12%|█▏        | 15/125 [01:03<08:26,  4.60s/it][A
 13%|█▎        | 16/125 [01:08<08:21,  4.60s/it][A
 14%|█▎        | 17/125 [01:13<08:16,  4.59s/it][A
 14%|█▍        | 18/125 [01:17<08:11,  4.59s/it][A
 15%|█▌        | 19/125 [01:22<08:06,  4.59s/it][A
 16%|█▌        | 20/125 [01:26<08:01,  4.58s/it][A
 17%|█▋        | 21/125 [01:31<07:57,  4.59s/it][A
 18%|█▊        | 22/125 [01:36<07:53,  4.59s/it][A
 18%|█▊        | 23/125 [01:40<07:48,  4.60s/it][A
 19%|█▉        | 24/125 [01:45<07:44,  4.60s/it][A
 20%|██        | 25/125 [01:49<07:40,  4.61s/it][A
 21%|██        | 26/125 [01:54<07:34,  4.59s/it][A
 22%|██▏       | 27/125 [01:59<07:30,  4.59s/it][A
 22%|██▏       | 28/125 [02:03<07:25,  4.59s/it][A
 23%|██▎       | 29/125 [02:08<07:20,  4.59s/it][A
 24%|██▍       | 30/125 [02:12<07:17,  4.60s/it][A
 25%|██▍       | 31/125 [02:17<07:12,  4.60s/it][A
 26%|██▌       | 32/125 [02:21<07:06,  4.59s/it][A
 26%|██▋       | 33/125 [02:26<07:03,  4.60s/it][A
 27%|██▋       | 34/125 [02:31<06:59,  4.61s/it][A
 28%|██▊       | 35/125 [02:35<06:53,  4.60s/it][A
 29%|██▉       | 36/125 [02:40<06:49,  4.60s/it][A
 30%|██▉       | 37/125 [02:44<06:44,  4.60s/it][A
 30%|███       | 38/125 [02:49<06:39,  4.59s/it][A
 31%|███       | 39/125 [02:54<06:34,  4.58s/it][A
 32%|███▏      | 40/125 [02:58<06:30,  4.59s/it][A
 33%|███▎      | 41/125 [03:03<06:25,  4.59s/it][A
 34%|███▎      | 42/125 [03:07<06:19,  4.58s/it][A
 34%|███▍      | 43/125 [03:12<06:17,  4.60s/it][A
 35%|███▌      | 44/125 [03:17<06:11,  4.59s/it][A
 36%|███▌      | 45/125 [03:21<06:06,  4.58s/it][A
 37%|███▋      | 46/125 [03:26<06:01,  4.58s/it][A
 38%|███▊      | 47/125 [03:30<05:57,  4.58s/it][A
 38%|███▊      | 48/125 [03:35<05:54,  4.60s/it][A
 39%|███▉      | 49/125 [03:40<05:50,  4.61s/it][A
 40%|████      | 50/125 [03:44<05:45,  4.61s/it][A
 41%|████      | 51/125 [03:49<05:42,  4.63s/it][A
 42%|████▏     | 52/125 [03:54<05:39,  4.65s/it][A
 42%|████▏     | 53/125 [03:58<05:33,  4.63s/it][A
 43%|████▎     | 54/125 [04:03<05:27,  4.61s/it][A
 44%|████▍     | 55/125 [04:07<05:22,  4.61s/it][A
 45%|████▍     | 56/125 [04:12<05:17,  4.60s/it][A
 46%|████▌     | 57/125 [04:17<05:13,  4.61s/it][A
 46%|████▋     | 58/125 [04:21<05:08,  4.61s/it][A
 47%|████▋     | 59/125 [04:26<05:03,  4.60s/it][A
 48%|████▊     | 60/125 [04:30<04:59,  4.60s/it][A
 49%|████▉     | 61/125 [04:35<04:54,  4.61s/it][A
 50%|████▉     | 62/125 [04:40<04:49,  4.60s/it][A
 50%|█████     | 63/125 [04:44<04:45,  4.60s/it][A
 51%|█████     | 64/125 [04:49<04:40,  4.60s/it][A
 52%|█████▏    | 65/125 [04:53<04:36,  4.60s/it][A
 53%|█████▎    | 66/125 [04:58<04:30,  4.58s/it][A
 54%|█████▎    | 67/125 [05:02<04:24,  4.57s/it][A
 54%|█████▍    | 68/125 [05:07<04:20,  4.57s/it][A
 55%|█████▌    | 69/125 [05:12<04:15,  4.57s/it][A
 56%|█████▌    | 70/125 [05:16<04:11,  4.57s/it][A
 57%|█████▋    | 71/125 [05:21<04:08,  4.59s/it][A
 58%|█████▊    | 72/125 [05:25<04:02,  4.58s/it][A
 58%|█████▊    | 73/125 [05:30<03:59,  4.60s/it][A
 59%|█████▉    | 74/125 [05:35<03:55,  4.61s/it][A
 60%|██████    | 75/125 [05:39<03:51,  4.63s/it][A
 61%|██████    | 76/125 [05:44<03:47,  4.63s/it][A
 62%|██████▏   | 77/125 [05:49<03:42,  4.63s/it][A
 62%|██████▏   | 78/125 [05:53<03:37,  4.62s/it][A
 63%|██████▎   | 79/125 [05:58<03:33,  4.63s/it][A
 64%|██████▍   | 80/125 [06:02<03:29,  4.65s/it][A
 65%|██████▍   | 81/125 [06:07<03:24,  4.64s/it][A
 66%|██████▌   | 82/125 [06:12<03:18,  4.63s/it][A
 66%|██████▋   | 83/125 [06:16<03:13,  4.60s/it][A
 67%|██████▋   | 84/125 [06:21<03:08,  4.59s/it][A
 68%|██████▊   | 85/125 [06:25<03:03,  4.58s/it][A
 69%|██████▉   | 86/125 [06:30<02:58,  4.58s/it][A
 70%|██████▉   | 87/125 [06:35<02:54,  4.60s/it][A
 70%|███████   | 88/125 [06:39<02:51,  4.62s/it][A
 71%|███████   | 89/125 [06:44<02:46,  4.63s/it][A
 72%|███████▏  | 90/125 [06:48<02:41,  4.61s/it][A
 73%|███████▎  | 91/125 [06:53<02:37,  4.62s/it][A
 74%|███████▎  | 92/125 [06:58<02:31,  4.60s/it][A
 74%|███████▍  | 93/125 [07:02<02:26,  4.58s/it][A
 75%|███████▌  | 94/125 [07:07<02:22,  4.60s/it][A
 76%|███████▌  | 95/125 [07:12<02:18,  4.62s/it][A
 77%|███████▋  | 96/125 [07:16<02:13,  4.60s/it][A
 78%|███████▊  | 97/125 [07:21<02:08,  4.59s/it][A
 78%|███████▊  | 98/125 [07:25<02:03,  4.59s/it][A
 79%|███████▉  | 99/125 [07:30<01:59,  4.60s/it][A
 80%|████████  | 100/125 [07:34<01:54,  4.59s/it][A
 81%|████████  | 101/125 [07:39<01:50,  4.58s/it][A
 82%|████████▏ | 102/125 [07:44<01:45,  4.59s/it][A
 82%|████████▏ | 103/125 [07:48<01:41,  4.61s/it][A
 83%|████████▎ | 104/125 [07:53<01:37,  4.62s/it][A
 84%|████████▍ | 105/125 [07:58<01:32,  4.63s/it][A
 85%|████████▍ | 106/125 [08:02<01:27,  4.62s/it][A
 86%|████████▌ | 107/125 [08:07<01:23,  4.62s/it][A
 86%|████████▋ | 108/125 [08:11<01:18,  4.63s/it][A
 87%|████████▋ | 109/125 [08:16<01:13,  4.61s/it][A
 88%|████████▊ | 110/125 [08:21<01:09,  4.60s/it][A
 89%|████████▉ | 111/125 [08:25<01:04,  4.60s/it][A
 90%|████████▉ | 112/125 [08:30<00:59,  4.60s/it][A
 90%|█████████ | 113/125 [08:34<00:55,  4.60s/it][A
 91%|█████████ | 114/125 [08:39<00:50,  4.59s/it][A
 92%|█████████▏| 115/125 [08:44<00:45,  4.60s/it][A
 93%|█████████▎| 116/125 [08:48<00:41,  4.60s/it][A
 94%|█████████▎| 117/125 [08:53<00:36,  4.61s/it][A
 94%|█████████▍| 118/125 [08:57<00:32,  4.61s/it][A
 95%|█████████▌| 119/125 [09:02<00:27,  4.62s/it][A
 96%|█████████▌| 120/125 [09:07<00:23,  4.60s/it][A
 97%|█████████▋| 121/125 [09:11<00:18,  4.59s/it][A
 98%|█████████▊| 122/125 [09:16<00:13,  4.58s/it][A
 98%|█████████▊| 123/125 [09:20<00:09,  4.60s/it][A
 99%|█████████▉| 124/125 [09:25<00:04,  4.61s/it][A
100%|██████████| 125/125 [09:30<00:00,  4.61s/it][A                                                      
                                                 [A 81%|████████▏ | 400/492 [1:45:26<2:54:35, 113.87s/it]
100%|██████████| 125/125 [09:30<00:00,  4.61s/it][A
                                                 [A/workspace/venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
wandb: Adding directory to artifact (./model_training/deepseek_prover_base_no_err/checkpoints-by_file-09-07-08-59/checkpoint-400)... Done. 4.2s
/workspace/venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 82%|████████▏ | 401/492 [1:47:31<7:19:16, 289.63s/it] 82%|████████▏ | 402/492 [1:49:25<5:55:24, 236.94s/it] 82%|████████▏ | 403/492 [1:51:19<4:56:53, 200.16s/it] 82%|████████▏ | 404/492 [1:53:13<4:15:33, 174.24s/it] 82%|████████▏ | 405/492 [1:55:07<3:46:29, 156.20s/it]                                                       82%|████████▏ | 405/492 [1:55:07<3:46:29, 156.20s/it] 83%|████████▎ | 406/492 [1:57:01<3:25:53, 143.65s/it] 83%|████████▎ | 407/492 [1:58:55<3:10:55, 134.77s/it] 83%|████████▎ | 408/492 [2:00:49<2:59:50, 128.45s/it] 83%|████████▎ | 409/492 [2:02:43<2:51:54, 124.28s/it] 83%|████████▎ | 410/492 [2:04:37<2:45:35, 121.16s/it]                                                       83%|████████▎ | 410/492 [2:04:37<2:45:35, 121.16s/it] 84%|████████▎ | 411/492 [2:06:32<2:40:52, 119.16s/it] 84%|████████▎ | 412/492 [2:08:26<2:36:52, 117.66s/it] 84%|████████▍ | 413/492 [2:10:20<2:33:27, 116.55s/it] 84%|████████▍ | 414/492 [2:12:14<2:30:32, 115.81s/it] 84%|████████▍ | 415/492 [2:14:08<2:28:01, 115.34s/it]                                                       84%|████████▍ | 415/492 [2:14:08<2:28:01, 115.34s/it] 85%|████████▍ | 416/492 [2:16:02<2:25:32, 114.90s/it] 85%|████████▍ | 417/492 [2:17:56<2:23:22, 114.70s/it] 85%|████████▍ | 418/492 [2:19:50<2:21:04, 114.38s/it] 85%|████████▌ | 419/492 [2:21:44<2:19:08, 114.36s/it] 85%|████████▌ | 420/492 [2:23:38<2:17:03, 114.22s/it]                                                       85%|████████▌ | 420/492 [2:23:38<2:17:03, 114.22s/it] 86%|████████▌ | 421/492 [2:25:32<2:14:51, 113.97s/it] 86%|████████▌ | 422/492 [2:27:25<2:12:49, 113.85s/it] 86%|████████▌ | 423/492 [2:29:19<2:10:55, 113.85s/it] 86%|████████▌ | 424/492 [2:31:13<2:09:07, 113.94s/it] 86%|████████▋ | 425/492 [2:33:08<2:07:32, 114.21s/it]                                                       86%|████████▋ | 425/492 [2:33:08<2:07:32, 114.21s/it] 87%|████████▋ | 426/492 [2:35:02<2:05:36, 114.18s/it] 87%|████████▋ | 427/492 [2:36:56<2:03:38, 114.14s/it] 87%|████████▋ | 428/492 [2:38:50<2:01:35, 114.00s/it] 87%|████████▋ | 429/492 [2:40:44<1:59:44, 114.05s/it] 87%|████████▋ | 430/492 [2:42:38<1:57:46, 113.98s/it]                                                       87%|████████▋ | 430/492 [2:42:38<1:57:46, 113.98s/it] 88%|████████▊ | 431/492 [2:44:31<1:55:45, 113.86s/it] 88%|████████▊ | 432/492 [2:46:25<1:53:53, 113.90s/it] 88%|████████▊ | 433/492 [2:48:20<1:52:05, 113.99s/it] 88%|████████▊ | 434/492 [2:50:14<1:50:11, 113.99s/it] 88%|████████▊ | 435/492 [2:52:08<1:48:22, 114.07s/it]                                                       88%|████████▊ | 435/492 [2:52:08<1:48:22, 114.07s/it] 89%|████████▊ | 436/492 [2:54:02<1:46:24, 114.01s/it] 89%|████████▉ | 437/492 [2:55:56<1:44:31, 114.03s/it] 89%|████████▉ | 438/492 [2:57:49<1:42:31, 113.91s/it] 89%|████████▉ | 439/492 [2:59:43<1:40:29, 113.77s/it] 89%|████████▉ | 440/492 [3:01:37<1:38:45, 113.96s/it]                                                       89%|████████▉ | 440/492 [3:01:37<1:38:45, 113.96s/it] 90%|████████▉ | 441/492 [3:03:31<1:36:45, 113.84s/it] 90%|████████▉ | 442/492 [3:05:25<1:34:54, 113.88s/it] 90%|█████████ | 443/492 [3:07:18<1:32:55, 113.78s/it] 90%|█████████ | 444/492 [3:09:12<1:31:05, 113.86s/it] 90%|█████████ | 445/492 [3:11:07<1:29:15, 113.94s/it]                                                       90%|█████████ | 445/492 [3:11:07<1:29:15, 113.94s/it] 91%|█████████ | 446/492 [3:13:01<1:27:23, 113.99s/it] 91%|█████████ | 447/492 [3:14:54<1:25:23, 113.84s/it] 91%|█████████ | 448/492 [3:16:48<1:23:35, 113.98s/it] 91%|█████████▏| 449/492 [3:18:43<1:21:42, 114.02s/it] 91%|█████████▏| 450/492 [3:20:37<1:19:52, 114.10s/it]                                                       91%|█████████▏| 450/492 [3:20:37<1:19:52, 114.10s/it]{'eval_loss': 0.14250527322292328, 'eval_runtime': 575.342, 'eval_samples_per_second': 1.738, 'eval_steps_per_second': 0.217, 'epoch': 3.25}
{'loss': 0.1075, 'grad_norm': 0.1152808740735054, 'learning_rate': 9.258712672491415e-06, 'epoch': 3.29}
{'loss': 0.1055, 'grad_norm': 0.0870693102478981, 'learning_rate': 8.254563928638893e-06, 'epoch': 3.33}
{'loss': 0.1162, 'grad_norm': 0.1064496636390686, 'learning_rate': 7.3031331341093915e-06, 'epoch': 3.37}
{'loss': 0.1032, 'grad_norm': 0.10074327141046524, 'learning_rate': 6.405621797022848e-06, 'epoch': 3.41}
{'loss': 0.1057, 'grad_norm': 0.11636592447757721, 'learning_rate': 5.563163333667099e-06, 'epoch': 3.45}
{'loss': 0.105, 'grad_norm': 0.08998982608318329, 'learning_rate': 4.776821637170526e-06, 'epoch': 3.49}
{'loss': 0.1032, 'grad_norm': 0.09578435868024826, 'learning_rate': 4.047589733971646e-06, 'epoch': 3.54}
{'loss': 0.1046, 'grad_norm': 0.10539178550243378, 'learning_rate': 3.376388529782215e-06, 'epoch': 3.58}
{'loss': 0.0981, 'grad_norm': 0.1038966253399849, 'learning_rate': 2.7640656466274782e-06, 'epoch': 3.62}
{'loss': 0.1014, 'grad_norm': 0.09623471647500992, 'learning_rate': 2.2113943524323167e-06, 'epoch': 3.66}

  0%|          | 0/125 [00:00<?, ?it/s][A
  2%|▏         | 2/125 [00:04<04:46,  2.33s/it][A
  2%|▏         | 3/125 [00:09<06:36,  3.25s/it][A
  3%|▎         | 4/125 [00:13<07:31,  3.73s/it][A
  4%|▍         | 5/125 [00:18<08:01,  4.01s/it][A
  5%|▍         | 6/125 [00:22<08:20,  4.21s/it][A
  6%|▌         | 7/125 [00:27<08:29,  4.32s/it][A
  6%|▋         | 8/125 [00:32<08:34,  4.40s/it][A
  7%|▋         | 9/125 [00:36<08:38,  4.47s/it][A
  8%|▊         | 10/125 [00:41<08:37,  4.50s/it][A
  9%|▉         | 11/125 [00:45<08:36,  4.53s/it][A
 10%|▉         | 12/125 [00:50<08:32,  4.54s/it][A
 10%|█         | 13/125 [00:54<08:30,  4.56s/it][A
 11%|█         | 14/125 [00:59<08:30,  4.60s/it][A
 12%|█▏        | 15/125 [01:04<08:26,  4.60s/it][A
 13%|█▎        | 16/125 [01:08<08:21,  4.60s/it][A
 14%|█▎        | 17/125 [01:13<08:16,  4.59s/it][A
 14%|█▍        | 18/125 [01:18<08:11,  4.59s/it][A
 15%|█▌        | 19/125 [01:22<08:06,  4.59s/it][A
 16%|█▌        | 20/125 [01:27<08:01,  4.59s/it][A
 17%|█▋        | 21/125 [01:31<07:57,  4.59s/it][A
 18%|█▊        | 22/125 [01:36<07:53,  4.59s/it][A
 18%|█▊        | 23/125 [01:41<07:48,  4.60s/it][A
 19%|█▉        | 24/125 [01:45<07:44,  4.60s/it][A
 20%|██        | 25/125 [01:50<07:40,  4.61s/it][A
 21%|██        | 26/125 [01:54<07:34,  4.59s/it][A
 22%|██▏       | 27/125 [01:59<07:30,  4.60s/it][A
 22%|██▏       | 28/125 [02:03<07:25,  4.60s/it][A
 23%|██▎       | 29/125 [02:08<07:20,  4.59s/it][A
 24%|██▍       | 30/125 [02:13<07:17,  4.61s/it][A
 25%|██▍       | 31/125 [02:17<07:12,  4.60s/it][A
 26%|██▌       | 32/125 [02:22<07:07,  4.59s/it][A
 26%|██▋       | 33/125 [02:27<07:03,  4.60s/it][A
 27%|██▋       | 34/125 [02:31<06:59,  4.61s/it][A
 28%|██▊       | 35/125 [02:36<06:54,  4.60s/it][A
 29%|██▉       | 36/125 [02:40<06:49,  4.60s/it][A
 30%|██▉       | 37/125 [02:45<06:44,  4.60s/it][A
 30%|███       | 38/125 [02:49<06:39,  4.59s/it][A
 31%|███       | 39/125 [02:54<06:34,  4.59s/it][A
 32%|███▏      | 40/125 [02:59<06:30,  4.59s/it][A
 33%|███▎      | 41/125 [03:03<06:25,  4.59s/it][A
 34%|███▎      | 42/125 [03:08<06:19,  4.58s/it][A
 34%|███▍      | 43/125 [03:12<06:17,  4.60s/it][A
 35%|███▌      | 44/125 [03:17<06:11,  4.59s/it][A
 36%|███▌      | 45/125 [03:22<06:06,  4.58s/it][A
 37%|███▋      | 46/125 [03:26<06:01,  4.58s/it][A
 38%|███▊      | 47/125 [03:31<05:57,  4.58s/it][A
 38%|███▊      | 48/125 [03:35<05:54,  4.60s/it][A
 39%|███▉      | 49/125 [03:40<05:50,  4.61s/it][A
 40%|████      | 50/125 [03:45<05:45,  4.61s/it][A
 41%|████      | 51/125 [03:49<05:42,  4.63s/it][A
 42%|████▏     | 52/125 [03:54<05:39,  4.65s/it][A
 42%|████▏     | 53/125 [03:59<05:33,  4.63s/it][A
 43%|████▎     | 54/125 [04:03<05:27,  4.61s/it][A
 44%|████▍     | 55/125 [04:08<05:22,  4.61s/it][A
 45%|████▍     | 56/125 [04:12<05:17,  4.60s/it][A
 46%|████▌     | 57/125 [04:17<05:13,  4.61s/it][A
 46%|████▋     | 58/125 [04:22<05:08,  4.61s/it][A
 47%|████▋     | 59/125 [04:26<05:03,  4.60s/it][A
 48%|████▊     | 60/125 [04:31<04:59,  4.60s/it][A
 49%|████▉     | 61/125 [04:35<04:54,  4.61s/it][A
 50%|████▉     | 62/125 [04:40<04:49,  4.60s/it][A
 50%|█████     | 63/125 [04:45<04:45,  4.60s/it][A
 51%|█████     | 64/125 [04:49<04:40,  4.60s/it][A
 52%|█████▏    | 65/125 [04:54<04:36,  4.60s/it][A
 53%|█████▎    | 66/125 [04:58<04:30,  4.58s/it][A
 54%|█████▎    | 67/125 [05:03<04:25,  4.57s/it][A
 54%|█████▍    | 68/125 [05:07<04:20,  4.58s/it][A
 55%|█████▌    | 69/125 [05:12<04:15,  4.57s/it][A
 56%|█████▌    | 70/125 [05:17<04:11,  4.57s/it][A
 57%|█████▋    | 71/125 [05:21<04:08,  4.60s/it][A
 58%|█████▊    | 72/125 [05:26<04:02,  4.58s/it][A
 58%|█████▊    | 73/125 [05:30<03:59,  4.60s/it][A
 59%|█████▉    | 74/125 [05:35<03:55,  4.61s/it][A
 60%|██████    | 75/125 [05:40<03:51,  4.63s/it][A
 61%|██████    | 76/125 [05:44<03:47,  4.63s/it][A
 62%|██████▏   | 77/125 [05:49<03:42,  4.63s/it][A
 62%|██████▏   | 78/125 [05:54<03:37,  4.62s/it][A
 63%|██████▎   | 79/125 [05:58<03:33,  4.64s/it][A
 64%|██████▍   | 80/125 [06:03<03:29,  4.65s/it][A
 65%|██████▍   | 81/125 [06:08<03:24,  4.64s/it][A
 66%|██████▌   | 82/125 [06:12<03:18,  4.63s/it][A
 66%|██████▋   | 83/125 [06:17<03:13,  4.61s/it][A
 67%|██████▋   | 84/125 [06:21<03:08,  4.59s/it][A
 68%|██████▊   | 85/125 [06:26<03:03,  4.58s/it][A
 69%|██████▉   | 86/125 [06:30<02:58,  4.58s/it][A
 70%|██████▉   | 87/125 [06:35<02:54,  4.60s/it][A
 70%|███████   | 88/125 [06:40<02:51,  4.62s/it][A
 71%|███████   | 89/125 [06:44<02:46,  4.63s/it][A
 72%|███████▏  | 90/125 [06:49<02:41,  4.61s/it][A
 73%|███████▎  | 91/125 [06:54<02:37,  4.62s/it][A
 74%|███████▎  | 92/125 [06:58<02:31,  4.60s/it][A
 74%|███████▍  | 93/125 [07:03<02:26,  4.59s/it][A
 75%|███████▌  | 94/125 [07:07<02:22,  4.61s/it][A
 76%|███████▌  | 95/125 [07:12<02:18,  4.62s/it][A
 77%|███████▋  | 96/125 [07:17<02:13,  4.61s/it][A
 78%|███████▊  | 97/125 [07:21<02:08,  4.59s/it][A
 78%|███████▊  | 98/125 [07:26<02:03,  4.59s/it][A
 79%|███████▉  | 99/125 [07:30<01:59,  4.60s/it][A
 80%|████████  | 100/125 [07:35<01:54,  4.59s/it][A
 81%|████████  | 101/125 [07:39<01:50,  4.58s/it][A
 82%|████████▏ | 102/125 [07:44<01:45,  4.59s/it][A
 82%|████████▏ | 103/125 [07:49<01:41,  4.60s/it][A
 83%|████████▎ | 104/125 [07:53<01:37,  4.62s/it][A
 84%|████████▍ | 105/125 [07:58<01:32,  4.63s/it][A
 85%|████████▍ | 106/125 [08:03<01:27,  4.62s/it][A
 86%|████████▌ | 107/125 [08:07<01:23,  4.62s/it][A
 86%|████████▋ | 108/125 [08:12<01:18,  4.63s/it][A
 87%|████████▋ | 109/125 [08:16<01:13,  4.61s/it][A
 88%|████████▊ | 110/125 [08:21<01:09,  4.60s/it][A
 89%|████████▉ | 111/125 [08:26<01:04,  4.60s/it][A
 90%|████████▉ | 112/125 [08:30<00:59,  4.60s/it][A
 90%|█████████ | 113/125 [08:35<00:55,  4.60s/it][A
 91%|█████████ | 114/125 [08:39<00:50,  4.58s/it][A
 92%|█████████▏| 115/125 [08:44<00:45,  4.59s/it][A
 93%|█████████▎| 116/125 [08:49<00:41,  4.59s/it][A
 94%|█████████▎| 117/125 [08:53<00:36,  4.61s/it][A
 94%|█████████▍| 118/125 [08:58<00:32,  4.61s/it][A
 95%|█████████▌| 119/125 [09:03<00:27,  4.62s/it][A
 96%|█████████▌| 120/125 [09:07<00:22,  4.60s/it][A
 97%|█████████▋| 121/125 [09:12<00:18,  4.59s/it][A
 98%|█████████▊| 122/125 [09:16<00:13,  4.58s/it][A
 98%|█████████▊| 123/125 [09:21<00:09,  4.60s/it][A
 99%|█████████▉| 124/125 [09:25<00:04,  4.61s/it][A
100%|██████████| 125/125 [09:30<00:00,  4.61s/it][A                                                      
                                                 [A 91%|█████████▏| 450/492 [3:30:12<1:19:52, 114.10s/it]
100%|██████████| 125/125 [09:30<00:00,  4.61s/it][A
                                                 [A/workspace/venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
wandb: Adding directory to artifact (./model_training/deepseek_prover_base_no_err/checkpoints-by_file-09-07-08-59/checkpoint-450)... Done. 4.3s
/workspace/venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 92%|█████████▏| 451/492 [3:32:17<3:18:07, 289.93s/it] 92%|█████████▏| 452/492 [3:34:11<2:38:04, 237.12s/it] 92%|█████████▏| 453/492 [3:36:05<2:10:03, 200.09s/it] 92%|█████████▏| 454/492 [3:37:59<1:50:20, 174.23s/it] 92%|█████████▏| 455/492 [3:39:53<1:36:17, 156.16s/it]                                                       92%|█████████▏| 455/492 [3:39:53<1:36:17, 156.16s/it] 93%|█████████▎| 456/492 [3:41:46<1:26:02, 143.42s/it] 93%|█████████▎| 457/492 [3:43:40<1:18:27, 134.49s/it] 93%|█████████▎| 458/492 [3:45:34<1:12:44, 128.37s/it] 93%|█████████▎| 459/492 [3:47:28<1:08:17, 124.17s/it] 93%|█████████▎| 460/492 [3:49:22<1:04:35, 121.11s/it]                                                       93%|█████████▎| 460/492 [3:49:22<1:04:35, 121.11s/it] 94%|█████████▎| 461/492 [3:51:17<1:01:30, 119.06s/it] 94%|█████████▍| 462/492 [3:53:10<58:44, 117.50s/it]   94%|█████████▍| 463/492 [3:55:04<56:17, 116.46s/it] 94%|█████████▍| 464/492 [3:56:59<54:03, 115.82s/it] 95%|█████████▍| 465/492 [3:58:53<51:52, 115.28s/it]                                                     95%|█████████▍| 465/492 [3:58:53<51:52, 115.28s/it] 95%|█████████▍| 466/492 [4:00:47<49:49, 114.99s/it] 95%|█████████▍| 467/492 [4:02:42<47:51, 114.85s/it] 95%|█████████▌| 468/492 [4:04:36<45:51, 114.63s/it] 95%|█████████▌| 469/492 [4:06:30<43:50, 114.37s/it] 96%|█████████▌| 470/492 [4:08:24<41:56, 114.36s/it]                                                     96%|█████████▌| 470/492 [4:08:24<41:56, 114.36s/it] 96%|█████████▌| 471/492 [4:10:18<40:00, 114.32s/it] 96%|█████████▌| 472/492 [4:12:12<38:05, 114.27s/it] 96%|█████████▌| 473/492 [4:14:07<36:11, 114.30s/it] 96%|█████████▋| 474/492 [4:16:01<34:17, 114.29s/it] 97%|█████████▋| 475/492 [4:17:54<32:18, 114.06s/it]                                                     97%|█████████▋| 475/492 [4:17:54<32:18, 114.06s/it] 97%|█████████▋| 476/492 [4:19:48<30:24, 114.03s/it] 97%|█████████▋| 477/492 [4:21:43<28:31, 114.10s/it] 97%|█████████▋| 478/492 [4:23:37<26:36, 114.07s/it] 97%|█████████▋| 479/492 [4:25:31<24:43, 114.09s/it] 98%|█████████▊| 480/492 [4:27:25<22:50, 114.21s/it]                                                     98%|█████████▊| 480/492 [4:27:25<22:50, 114.21s/it] 98%|█████████▊| 481/492 [4:29:20<20:56, 114.27s/it] 98%|█████████▊| 482/492 [4:31:14<19:02, 114.25s/it] 98%|█████████▊| 483/492 [4:33:08<17:08, 114.23s/it] 98%|█████████▊| 484/492 [4:35:02<15:12, 114.07s/it] 99%|█████████▊| 485/492 [4:36:56<13:18, 114.08s/it]                                                     99%|█████████▊| 485/492 [4:36:56<13:18, 114.08s/it] 99%|█████████▉| 486/492 [4:38:50<11:24, 114.06s/it] 99%|█████████▉| 487/492 [4:40:44<09:29, 113.99s/it] 99%|█████████▉| 488/492 [4:42:38<07:36, 114.03s/it] 99%|█████████▉| 489/492 [4:44:31<05:41, 113.92s/it]100%|█████████▉| 490/492 [4:46:25<03:47, 113.91s/it]                                                    100%|█████████▉| 490/492 [4:46:25<03:47, 113.91s/it]100%|█████████▉| 491/492 [4:48:19<01:53, 113.85s/it]100%|██████████| 492/492 [4:50:13<00:00, 113.90s/it]/workspace/venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
wandb: Adding directory to artifact (./model_training/deepseek_prover_base_no_err/checkpoints-by_file-09-07-08-59/checkpoint-492)... Done. 8.6s
                                                    100%|██████████| 492/492 [4:50:35<00:00, 113.90s/it]/workspace/venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'eval_loss': 0.1424967348575592, 'eval_runtime': 575.2181, 'eval_samples_per_second': 1.738, 'eval_steps_per_second': 0.217, 'epoch': 3.66}
{'loss': 0.1024, 'grad_norm': 0.12714102864265442, 'learning_rate': 1.7190725845048827e-06, 'epoch': 3.7}
{'loss': 0.1029, 'grad_norm': 0.10180765390396118, 'learning_rate': 1.2877220681510927e-06, 'epoch': 3.74}
{'loss': 0.1059, 'grad_norm': 0.10621170699596405, 'learning_rate': 9.178875315329183e-07, 'epoch': 3.78}
{'loss': 0.1107, 'grad_norm': 0.10690198838710785, 'learning_rate': 6.100360177619946e-07, 'epoch': 3.82}
{'loss': 0.11, 'grad_norm': 0.08205580711364746, 'learning_rate': 3.6455629509730136e-07, 'epoch': 3.86}
{'loss': 0.115, 'grad_norm': 0.10496656596660614, 'learning_rate': 1.8175836599173546e-07, 'epoch': 3.9}
{'loss': 0.1058, 'grad_norm': 0.09522535651922226, 'learning_rate': 6.187307560754363e-08, 'epoch': 3.94}
{'loss': 0.1001, 'grad_norm': 0.11084733158349991, 'learning_rate': 5.051820295032261e-09, 'epoch': 3.98}
{'train_runtime': 17444.1924, 'train_samples_per_second': 1.806, 'train_steps_per_second': 0.028, 'train_loss': 0.030754005824162708, 'epoch': 4.0}
Finetuning finished.
wandb: - 4890.769 MB of 4890.769 MB uploaded (13.219 MB deduped)wandb: \ 4890.769 MB of 4890.769 MB uploaded (13.219 MB deduped)wandb: | 4890.769 MB of 4890.769 MB uploaded (13.219 MB deduped)wandb: / 4890.769 MB of 4890.769 MB uploaded (13.219 MB deduped)wandb: - 4890.769 MB of 4890.769 MB uploaded (13.219 MB deduped)wandb: \ 4890.793 MB of 4890.793 MB uploaded (13.219 MB deduped)wandb: | 4890.793 MB of 4890.793 MB uploaded (13.219 MB deduped)wandb: 
wandb: Run history:
wandb:               eval/loss █▁
wandb:            eval/runtime █▁
wandb: eval/samples_per_second ▁▁
wandb:   eval/steps_per_second ▁▁
wandb:             train/epoch ▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███
wandb:       train/global_step ▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███
wandb:         train/grad_norm ▇▄▂▆▄▆▄▅▂▂▆▂▅▄▆▂▃▅▄▃█▄▅▅▁▅▃▅
wandb:     train/learning_rate ██▇▇▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁
wandb:              train/loss ▃▄▄▆▄▂▇█▄▁▄▄▇▃▄▃▃▃▁▂▂▃▄▅▅▇▄▂
wandb: 
wandb: Run summary:
wandb:                eval/loss 0.1425
wandb:             eval/runtime 575.2181
wandb:  eval/samples_per_second 1.738
wandb:    eval/steps_per_second 0.217
wandb:               total_flos 3.138580840489943e+18
wandb:              train/epoch 3.99797
wandb:        train/global_step 492
wandb:          train/grad_norm 0.11085
wandb:      train/learning_rate 0.0
wandb:               train/loss 0.1001
wandb:               train_loss 0.03075
wandb:            train_runtime 17444.1924
wandb: train_samples_per_second 1.806
wandb:   train_steps_per_second 0.028
wandb: 
wandb: 🚀 View run deepseek_prover_base_no_err-by_file-09-07-08-59 at: https://wandb.ai/tcwong/decoder/runs/xjbmhng9
wandb: ⭐️ View project at: https://wandb.ai/tcwong/decoder
wandb: Synced 5 W&B file(s), 0 media file(s), 33 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240907_212339-xjbmhng9/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
