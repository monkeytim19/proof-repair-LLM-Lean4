{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from config import RAW_DATA_DIR\n",
    "from utils.theorem_extraction import file_theorem_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO need to remove after commmits are consistent\n",
    "from data_generation.git_extraction_helper import file_commits\n",
    "commit_sha = 'a261710852a957a7d20d89b962e4b59887549f21'\n",
    "all_commits = file_commits()\n",
    "temp = [c.hexsha for c in all_commits].index(commit_sha)\n",
    "REF_COMMIT = all_commits[temp]\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_error_msg_redundancies(error_msg):\n",
    "    \"\"\"Removes the unnecessary parts of the error message.\"\"\"\n",
    "    msg_by_lines = error_msg.splitlines()\n",
    "\n",
    "    for idx, line in enumerate(msg_by_lines):\n",
    "        # find the index of the line starting with 'trace: '\n",
    "        if line.strip().startswith('trace:'):\n",
    "            start_idx = idx + 1\n",
    "    \n",
    "        # Find the index of the line 'Some builds logged failures:'\n",
    "        if line == 'error: Lean exited with code 1':\n",
    "            end_idx = idx\n",
    "            break\n",
    "    \n",
    "    # extract lines between the two points\n",
    "    extracted_lines = '\\n'.join(msg_by_lines[start_idx: end_idx])\n",
    "    \n",
    "    return extracted_lines\n",
    "\n",
    "def get_ground_truth_info(df):\n",
    "    \"\"\"\n",
    "    Obtain the dictionary of the ground truth information on the theorems in the given dataset.\n",
    "\n",
    "    This contains the correct theorem statement and proof from the reference commit.\n",
    "    \"\"\"\n",
    "    thm_info_dict = {}\n",
    "    for filepath in df['filepath'].unique():\n",
    "        thm_names_ls = df['thm_name'][df['filepath'] == filepath]\n",
    "        fp_dict = file_theorem_info(REF_COMMIT, filepath, thm_names_ls)\n",
    "        thm_info_dict = thm_info_dict | fp_dict\n",
    "    return thm_info_dict\n",
    "\n",
    "def query_ground_truth(row, ground_truth_table):\n",
    "    \"\"\"\n",
    "    Wrapper function to help index and determine the ground truth for a given row in the dataset.\n",
    "    \"\"\"\n",
    "    return pd.Series(ground_truth_table[(row['filepath'], row['thm_name'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files = [f for f in os.listdir(RAW_DATA_DIR)]\n",
    "\n",
    "dataframes = []\n",
    "for json_file in json_files:\n",
    "    file_path = os.path.join(RAW_DATA_DIR, json_file)\n",
    "    df = pd.read_json(file_path)\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# remove duplicate rows\n",
    "df = df.drop_duplicates(subset=['filepath', 'thm_name', 'failed_proof', 'error_msg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files = [f for f in os.listdir(RAW_DATA_DIR)]\n",
    "\n",
    "dataframes = []\n",
    "for json_file in json_files:\n",
    "    file_path = os.path.join(RAW_DATA_DIR, json_file)\n",
    "    df = pd.read_json(file_path)\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# remove duplicate rows\n",
    "df = df.drop_duplicates(subset=['filepath', 'thm_name', 'failed_proof', 'error_msg'])\n",
    "\n",
    "# remove redundancies in error message\n",
    "df['error_msg'] = df['error_msg'].apply(remove_error_msg_redundancies)\n",
    "\n",
    "# append the ground truth information to the dataset\n",
    "ground_truth_table = get_ground_truth_info(df)\n",
    "df[['statement', 'proof']] = df.apply(lambda row: query_ground_truth(row, ground_truth_table), axis=1)\n",
    "# print(len(df))\n",
    "\n",
    "# remove all empty proofs\n",
    "df = df[df['failed_proof'] != '']\n",
    "df = df[df['proof'] != '']\n",
    "\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('processed_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = pd.read_csv('processed_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = k != df\n",
    "\n",
    "# identify rows with differences\n",
    "rows_with_differences = diff.any(axis=1)\n",
    "\n",
    "# extract specific rows where differences occur\n",
    "rows_diff_df1 = df[rows_with_differences]\n",
    "rows_diff_df2 = k[rows_with_differences]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
